{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module `pepper.metrics`**\n",
    "\n",
    "Aperçu des mesures qui requièrent `predict_proba` :\n",
    "\n",
    "1. **Log Loss (Logarithmic Loss)**\n",
    "    - Évalue la qualité des prédictions probabilistes.\n",
    "    - Elle est souvent utilisée pour évaluer des modèles de classification probabiliste.\n",
    "    - Elle est couramment utilisée dans des compétitions telles que Kaggle.\n",
    "2. **AUC-ROC (Area Under the Receiver Operating Characteristic Curve)** :\n",
    "    - AUC-ROC nécessite des scores de probabilité pour les classes positives.\n",
    "3. **AUC-PR (Area Under the Precision-Recall Curve)** :\n",
    "    - Évalue la qualité des prédictions positives dans un contexte de déséquilibre de classe.\n",
    "4. **F1 Score** :\n",
    "    - Bien que le F1-score puisse être calculé à partir de prédictions binaires, il est parfois calculé à partir de scores de probabilité en choisissant un seuil optimal.\n",
    "5. **Logistic Loss (Log Loss)**, ou perte logistique :\n",
    "    - Utilisée pour évaluer la performance de modèles de classification probabilistes.\n",
    "    - Mesure à quel point les prédictions probabilistes correspondent aux étiquettes réelles.\n",
    "6. **Brier Score Loss (Brier Score)** :\n",
    "    - Évalue la précision des prédictions probabilistes.\n",
    "    - Utilisée pour mesurer la distance entre les probabilités prédites et les vérités terrain pour les classes positives.\n",
    "7. **Average Precision Score** :\n",
    "    - Évalue la qualité des prédictions positives en mesurant la précision moyenne de la récupération (AP).\n",
    "8. **Precision-Recall Curve** :\n",
    "    - La construction de la courbe de précision-rappel nécessite des scores de probabilité pour estimer les valeurs de précision et de rappel à différents seuils de probabilité.\n",
    "\n",
    "Ces mesures sont couramment utilisées pour évaluer la performance des modèles de classification probabilistes, car elles prennent en compte la confiance des prédictions et non pas seulement les étiquettes de classe binaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`require_probas`**`(metric)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "from pepper.metrics import require_probas\n",
    "from sklearn import metrics\n",
    "\n",
    "# Test with metrics that require probabilistic predictions\n",
    "assert require_probas(metrics.roc_auc_score) == True\n",
    "assert require_probas(metrics.brier_score_loss) == True\n",
    "assert require_probas(metrics.average_precision_score) == True\n",
    "assert require_probas(metrics.precision_recall_curve) == True\n",
    "assert require_probas(metrics.log_loss) == True\n",
    "assert require_probas(metrics.f1_score) == True\n",
    "assert require_probas(metrics.log_loss) == True\n",
    "\n",
    "# Test with metrics that do not require probabilistic predictions\n",
    "assert require_probas(metrics.accuracy_score) == False\n",
    "assert require_probas(metrics.confusion_matrix) == False\n",
    "assert require_probas(metrics.matthews_corrcoef) == False\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
