{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Module `home_credit.model.facade`**\n",
    "\n",
    "**TODO** À la prochaine passe sur ce module en faire une classe de wrapper de modèles pour créer une couche d'abstraction qui nous permette d'utiliser indifféremment du SKL, du LGBM ou d'autres librairies dans une même application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`fit_facade`**`(clf, X_y_train, X_y_valid, loss_func)`\n",
    "\n",
    "Fit a classifier using the appropriate training method based on the classifier type.\n",
    "\n",
    "**Utilisée** par `kfold_train_and_eval_model`.\n",
    "\n",
    "Le but est de faire abstraction de la librairie utilisée (Scikit-learn, LightGBM, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4)\n",
      "y shape: (150,)\n",
      "X_train shape: (120, 4)\n",
      "X_valid shape: (30, 4)\n",
      "X_train shape: (120,)\n",
      "X_valid shape: (30,)\n",
      "y_true shape: (120,)\n",
      "y_pred shape: (360,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [360, 120]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_credit_scoring_tool\\notebooks\\_lib\\home_credit\\best_model_search.ipynb Cellule 5\u001b[0m line \u001b[0;36m3\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m clf \u001b[39m=\u001b[39m LGBMClassifier()\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X26sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Test fitting with the LightGBM model\u001b[39;00m\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X26sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m fit_facade(clf, (X_train, y_train), (X_valid, y_valid), custom_lgbm_log_loss)\n",
      "\n",
      "File \u001b[1;32m~\\Projects\\pepper_credit_scoring_tool\\python\\home_credit\\best_model_search.py:97\u001b[0m, in \u001b[0;36mfit_facade\u001b[1;34m(clf, X_y_train, X_y_valid, loss_func)\u001b[0m\n",
      "\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(clf, lgbm\u001b[39m.\u001b[39mLGBMClassifier):\n",
      "\u001b[0;32m     88\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     89\u001b[0m \u001b[39m    X_train: the training features\u001b[39;00m\n",
      "\u001b[0;32m     90\u001b[0m \u001b[39m    y_train: the training target variable\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     95\u001b[0m \u001b[39m    sample_weight: sample weights for each training sample\u001b[39;00m\n",
      "\u001b[0;32m     96\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m---> 97\u001b[0m     clf\u001b[39m.\u001b[39;49mfit(\n",
      "\u001b[0;32m     98\u001b[0m         \u001b[39m*\u001b[39;49mX_y_train,\n",
      "\u001b[0;32m     99\u001b[0m         eval_set\u001b[39m=\u001b[39;49m[X_y_train, X_y_valid],\n",
      "\u001b[0;32m    100\u001b[0m         eval_metric\u001b[39m=\u001b[39;49mloss_func,\n",
      "\u001b[0;32m    101\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m\n",
      "\u001b[0;32m    102\u001b[0m     )\n",
      "\u001b[0;32m    103\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(clf, ClassifierMixin):\n",
      "\u001b[0;32m    104\u001b[0m     clf\u001b[39m.\u001b[39mfit(\u001b[39m*\u001b[39mX_y_train)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n",
      "\u001b[0;32m    964\u001b[0m         \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[39m=\u001b[39m (valid_x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_le\u001b[39m.\u001b[39mtransform(valid_y))\n",
      "\u001b[1;32m--> 967\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, _y, sample_weight\u001b[39m=\u001b[39;49msample_weight, init_score\u001b[39m=\u001b[39;49minit_score, eval_set\u001b[39m=\u001b[39;49mvalid_sets,\n",
      "\u001b[0;32m    968\u001b[0m             eval_names\u001b[39m=\u001b[39;49meval_names, eval_sample_weight\u001b[39m=\u001b[39;49meval_sample_weight,\n",
      "\u001b[0;32m    969\u001b[0m             eval_class_weight\u001b[39m=\u001b[39;49meval_class_weight, eval_init_score\u001b[39m=\u001b[39;49meval_init_score,\n",
      "\u001b[0;32m    970\u001b[0m             eval_metric\u001b[39m=\u001b[39;49meval_metric, early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n",
      "\u001b[0;32m    971\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose, feature_name\u001b[39m=\u001b[39;49mfeature_name, categorical_feature\u001b[39m=\u001b[39;49mcategorical_feature,\n",
      "\u001b[0;32m    972\u001b[0m             callbacks\u001b[39m=\u001b[39;49mcallbacks, init_model\u001b[39m=\u001b[39;49minit_model)\n",
      "\u001b[0;32m    973\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n",
      "\u001b[0;32m    745\u001b[0m evals_result \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;32m    746\u001b[0m callbacks\u001b[39m.\u001b[39mappend(record_evaluation(evals_result))\n",
      "\u001b[1;32m--> 748\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n",
      "\u001b[0;32m    749\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n",
      "\u001b[0;32m    750\u001b[0m     train_set\u001b[39m=\u001b[39;49mtrain_set,\n",
      "\u001b[0;32m    751\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_estimators,\n",
      "\u001b[0;32m    752\u001b[0m     valid_sets\u001b[39m=\u001b[39;49mvalid_sets,\n",
      "\u001b[0;32m    753\u001b[0m     valid_names\u001b[39m=\u001b[39;49meval_names,\n",
      "\u001b[0;32m    754\u001b[0m     fobj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fobj,\n",
      "\u001b[0;32m    755\u001b[0m     feval\u001b[39m=\u001b[39;49meval_metrics_callable,\n",
      "\u001b[0;32m    756\u001b[0m     init_model\u001b[39m=\u001b[39;49minit_model,\n",
      "\u001b[0;32m    757\u001b[0m     feature_name\u001b[39m=\u001b[39;49mfeature_name,\n",
      "\u001b[0;32m    758\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks\n",
      "\u001b[0;32m    759\u001b[0m )\n",
      "\u001b[0;32m    761\u001b[0m \u001b[39mif\u001b[39;00m evals_result:\n",
      "\u001b[0;32m    762\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evals_result \u001b[39m=\u001b[39m evals_result\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\engine.py:298\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n",
      "\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m valid_sets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m    297\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n",
      "\u001b[1;32m--> 298\u001b[0m         evaluation_result_list\u001b[39m.\u001b[39mextend(booster\u001b[39m.\u001b[39;49meval_train(feval))\n",
      "\u001b[0;32m    299\u001b[0m     evaluation_result_list\u001b[39m.\u001b[39mextend(booster\u001b[39m.\u001b[39meval_valid(feval))\n",
      "\u001b[0;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:3238\u001b[0m, in \u001b[0;36mBooster.eval_train\u001b[1;34m(self, feval)\u001b[0m\n",
      "\u001b[0;32m   3207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_train\u001b[39m(\u001b[39mself\u001b[39m, feval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;32m   3208\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Evaluate for training data.\u001b[39;00m\n",
      "\u001b[0;32m   3209\u001b[0m \n",
      "\u001b[0;32m   3210\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   3236\u001b[0m \u001b[39m        List with evaluation results.\u001b[39;00m\n",
      "\u001b[0;32m   3237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 3238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__inner_eval(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_data_name, \u001b[39m0\u001b[39;49m, feval)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\basic.py:3809\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n",
      "\u001b[0;32m   3807\u001b[0m \u001b[39mif\u001b[39;00m eval_function \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m   3808\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32m-> 3809\u001b[0m feval_ret \u001b[39m=\u001b[39m eval_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__inner_predict(data_idx), cur_data)\n",
      "\u001b[0;32m   3810\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feval_ret, \u001b[39mlist\u001b[39m):\n",
      "\u001b[0;32m   3811\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, val, is_higher_better \u001b[39min\u001b[39;00m feval_ret:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\sklearn.py:180\u001b[0m, in \u001b[0;36m_EvalFunctionWrapper.__call__\u001b[1;34m(self, preds, dataset)\u001b[0m\n",
      "\u001b[0;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(labels, preds, dataset\u001b[39m.\u001b[39mget_weight())\n",
      "\u001b[0;32m    179\u001b[0m \u001b[39melif\u001b[39;00m argc \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "\u001b[1;32m--> 180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(labels, preds, dataset\u001b[39m.\u001b[39;49mget_weight(), dataset\u001b[39m.\u001b[39;49mget_group())\n",
      "\u001b[0;32m    181\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    182\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSelf-defined eval function should have 2, 3 or 4 arguments, got \u001b[39m\u001b[39m{\u001b[39;00margc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_credit_scoring_tool\\notebooks\\_lib\\home_credit\\best_model_search.ipynb Cellule 5\u001b[0m line \u001b[0;36m1\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true shape: \u001b[39m\u001b[39m{\u001b[39;00my_true\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred shape: \u001b[39m\u001b[39m{\u001b[39;00my_pred\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m log_loss(y_true, y_pred, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n",
      "\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n",
      "\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n",
      "\u001b[0;32m    209\u001b[0m         )\n",
      "\u001b[0;32m    210\u001b[0m     ):\n",
      "\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n",
      "\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n",
      "\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n",
      "\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n",
      "\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n",
      "\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n",
      "\u001b[0;32m    221\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2854\u001b[0m, in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n",
      "\u001b[0;32m   2843\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m   2844\u001b[0m     \u001b[39m# TODO: Remove user defined eps in 1.5\u001b[39;00m\n",
      "\u001b[0;32m   2845\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n",
      "\u001b[0;32m   2846\u001b[0m         (\n",
      "\u001b[0;32m   2847\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSetting the eps parameter is deprecated and will \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   2851\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n",
      "\u001b[0;32m   2852\u001b[0m     )\n",
      "\u001b[1;32m-> 2854\u001b[0m check_consistent_length(y_pred, y_true, sample_weight)\n",
      "\u001b[0;32m   2855\u001b[0m lb \u001b[39m=\u001b[39m LabelBinarizer()\n",
      "\u001b[0;32m   2857\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n",
      "\u001b[0;32m    407\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n",
      "\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;32m--> 409\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    410\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    411\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n",
      "\u001b[0;32m    412\u001b[0m     )\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [360, 120]"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from lightgbm import LGBMClassifier\n",
    "import pandas as pd\n",
    "\n",
    "from home_credit.model.facade import fit_facade\n",
    "\n",
    "def custom_lgbm_log_loss(y_true: pd.Series, y_pred: pd.Series, sample_weight, group):\n",
    "    # Utiliser log_loss à l'intérieur de cette fonction\n",
    "    print(f\"y_true shape: {y_true.shape}\")\n",
    "    print(f\"y_pred shape: {y_pred.shape}\")\n",
    "    return log_loss(y_true, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "# Create a dummy dataset for the example\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"X_train shape: {y_train.shape}\")\n",
    "print(f\"X_valid shape: {y_valid.shape}\")\n",
    "\n",
    "# Initialize a LightGBM model\n",
    "clf = LGBMClassifier()\n",
    "\n",
    "# Test fitting with the LightGBM model\n",
    "fit_facade(clf, (X_train, y_train), (X_valid, y_valid), custom_lgbm_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from home_credit.model.facade import fit_facade\n",
    "\n",
    "# Create a dummy dataset for the example\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a scikit-learn model\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Test fitting with the scikit-learn model\n",
    "fit_facade(clf, (X_train, y_train), (X_valid, y_valid), log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`predict_facade`**`(clf, X)`\n",
    "\n",
    "Facade function for making predictions using a classifier.\n",
    "\n",
    "**Utilisée** par `kfold_train_and_eval_model`.\n",
    "\n",
    "Le but est de faire abstraction de la librairie utilisée (Scikit-learn, LightGBM, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with NumPy array input:\n",
      "[0 1]\n",
      "\n",
      "Predictions with Pandas DataFrame input:\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from home_credit.model.facade import predict_facade\n",
    "\n",
    "# Create a dummy dataset for testing\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a LightGBM model and fit it\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the predict_facade function with a NumPy array\n",
    "X_test_np = np.array([[5.1, 3.5, 1.4, 0.2], [6.2, 2.9, 4.3, 1.3]])\n",
    "predictions_np = predict_facade(clf, X_test_np)\n",
    "print(\"Predictions with NumPy array input:\")\n",
    "print(predictions_np)\n",
    "\n",
    "# Test the predict_facade function with a Pandas DataFrame\n",
    "X_test_df = pd.DataFrame({\n",
    "    'feature1': [5.1, 6.2],\n",
    "    'feature2': [3.5, 2.9],\n",
    "    'feature3': [1.4, 4.3],\n",
    "    'feature4': [0.2, 1.3]\n",
    "})\n",
    "\n",
    "predictions_df = predict_facade(clf, X_test_df)\n",
    "print(\"\\nPredictions with Pandas DataFrame input:\")\n",
    "print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`predict_proba_facade`**`(clf, X)`\n",
    "\n",
    "Predict class probabilities using a classifier.\n",
    "\n",
    "**Utilisée** par `kfold_train_and_eval_model`.\n",
    "\n",
    "Le but est de faire abstraction de la librairie utilisée (Scikit-learn, LightGBM, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "No best_iteration found. Need to call fit with early_stopping callback beforehand.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\franc\\Projects\\pepper_credit_scoring_tool\\notebooks\\_lib\\home_credit\\best_model_search.ipynb Cellule 10\u001b[0m line \u001b[0;36m1\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m5.1\u001b[39m, \u001b[39m3.5\u001b[39m, \u001b[39m1.4\u001b[39m, \u001b[39m0.2\u001b[39m], [\u001b[39m6.2\u001b[39m, \u001b[39m2.9\u001b[39m, \u001b[39m4.3\u001b[39m, \u001b[39m1.3\u001b[39m]])\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/franc/Projects/pepper_credit_scoring_tool/notebooks/_lib/home_credit/best_model_search.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m predictions \u001b[39m=\u001b[39m predict_proba_facade(clf, X_test)\n",
      "\n",
      "File \u001b[1;32m~\\Projects\\pepper_credit_scoring_tool\\python\\home_credit\\best_model_search.py:118\u001b[0m, in \u001b[0;36mpredict_proba_facade\u001b[1;34m(clf, X)\u001b[0m\n",
      "\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_facade\u001b[39m(\n",
      "\u001b[0;32m     87\u001b[0m     clf: Union[lgbm\u001b[39m.\u001b[39mLGBMClassifier, ClassifierMixin],\n",
      "\u001b[0;32m     88\u001b[0m     X_y_train: Tuple[pd\u001b[39m.\u001b[39mDataFrame, pd\u001b[39m.\u001b[39mSeries],\n",
      "\u001b[0;32m     89\u001b[0m     X_y_valid: Tuple[pd\u001b[39m.\u001b[39mDataFrame, pd\u001b[39m.\u001b[39mSeries],\n",
      "\u001b[0;32m     90\u001b[0m     loss_func: Callable\n",
      "\u001b[0;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     93\u001b[0m \u001b[39m    Fit a classifier using the appropriate training method\u001b[39;00m\n",
      "\u001b[0;32m     94\u001b[0m \u001b[39m    based on the classifier type.\u001b[39;00m\n",
      "\u001b[0;32m     95\u001b[0m \n",
      "\u001b[0;32m     96\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n",
      "\u001b[0;32m     97\u001b[0m \u001b[39m    -----------\u001b[39;00m\n",
      "\u001b[0;32m     98\u001b[0m \u001b[39m    clf : Union[lgbm.LGBMClassifier, ClassifierMixin]\u001b[39;00m\n",
      "\u001b[0;32m     99\u001b[0m \u001b[39m        The classifier to fit. Should be either a LightGBM classifier\u001b[39;00m\n",
      "\u001b[0;32m    100\u001b[0m \u001b[39m        or a scikit-learn compatible classifier.\u001b[39;00m\n",
      "\u001b[0;32m    101\u001b[0m \n",
      "\u001b[0;32m    102\u001b[0m \u001b[39m    X_y_train : Tuple[pd.DataFrame, pd.Series]\u001b[39;00m\n",
      "\u001b[0;32m    103\u001b[0m \u001b[39m        A tuple containing the training features\u001b[39;00m\n",
      "\u001b[0;32m    104\u001b[0m \u001b[39m        and the corresponding target variable.\u001b[39;00m\n",
      "\u001b[0;32m    105\u001b[0m \n",
      "\u001b[0;32m    106\u001b[0m \u001b[39m    X_y_valid : Tuple[pd.DataFrame, pd.Series]\u001b[39;00m\n",
      "\u001b[0;32m    107\u001b[0m \u001b[39m        A tuple containing the validation features\u001b[39;00m\n",
      "\u001b[0;32m    108\u001b[0m \u001b[39m        and the corresponding target variable.\u001b[39;00m\n",
      "\u001b[0;32m    109\u001b[0m \n",
      "\u001b[0;32m    110\u001b[0m \u001b[39m    loss_func : Callable\u001b[39;00m\n",
      "\u001b[0;32m    111\u001b[0m \u001b[39m        The loss function used for evaluation during training.\u001b[39;00m\n",
      "\u001b[0;32m    112\u001b[0m \n",
      "\u001b[0;32m    113\u001b[0m \u001b[39m    Raises:\u001b[39;00m\n",
      "\u001b[0;32m    114\u001b[0m \u001b[39m    -------\u001b[39;00m\n",
      "\u001b[0;32m    115\u001b[0m \u001b[39m    ValueError\u001b[39;00m\n",
      "\u001b[0;32m    116\u001b[0m \u001b[39m        If an invalid classifier type is provided.\u001b[39;00m\n",
      "\u001b[0;32m    117\u001b[0m \n",
      "\u001b[1;32m--> 118\u001b[0m \u001b[39m    Notes:\u001b[39;00m\n",
      "\u001b[0;32m    119\u001b[0m \u001b[39m    ------\u001b[39;00m\n",
      "\u001b[0;32m    120\u001b[0m \u001b[39m    - If the classifier is an LGBMClassifier, it will use early stopping during\u001b[39;00m\n",
      "\u001b[0;32m    121\u001b[0m \u001b[39m        training with evaluation sets provided by `X_y_train` and `X_y_valid`.\u001b[39;00m\n",
      "\u001b[0;32m    122\u001b[0m \u001b[39m    - If the classifier is a scikit-learn compatible classifier,\u001b[39;00m\n",
      "\u001b[0;32m    123\u001b[0m \u001b[39m        it will fit the model without early stopping.\u001b[39;00m\n",
      "\u001b[0;32m    124\u001b[0m \n",
      "\u001b[0;32m    125\u001b[0m \u001b[39m    Example:\u001b[39;00m\n",
      "\u001b[0;32m    126\u001b[0m \u001b[39m    --------\u001b[39;00m\n",
      "\u001b[0;32m    127\u001b[0m \u001b[39m    >>> from lightgbm import LGBMClassifier\u001b[39;00m\n",
      "\u001b[0;32m    128\u001b[0m \u001b[39m    >>> from sklearn.datasets import load_iris\u001b[39;00m\n",
      "\u001b[0;32m    129\u001b[0m \u001b[39m    >>> from sklearn.model_selection import train_test_split\u001b[39;00m\n",
      "\u001b[0;32m    130\u001b[0m \u001b[39m    >>> from sklearn.metrics import log_loss\u001b[39;00m\n",
      "\u001b[0;32m    131\u001b[0m \u001b[39m    >>> X, y = load_iris(return_X_y=True)\u001b[39;00m\n",
      "\u001b[0;32m    132\u001b[0m \u001b[39m    >>> X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\u001b[39;00m\n",
      "\u001b[0;32m    133\u001b[0m \u001b[39m    >>> clf = LGBMClassifier()\u001b[39;00m\n",
      "\u001b[0;32m    134\u001b[0m \u001b[39m    >>> fit_facade(clf, (X_train, y_train), (X_valid, y_valid), log_loss)\u001b[39;00m\n",
      "\u001b[0;32m    135\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(clf, lgbm\u001b[39m.\u001b[39mLGBMClassifier):\n",
      "\u001b[0;32m    137\u001b[0m         clf\u001b[39m.\u001b[39mfit(\n",
      "\u001b[0;32m    138\u001b[0m             \u001b[39m*\u001b[39mX_y_train,\n",
      "\u001b[0;32m    139\u001b[0m             eval_set\u001b[39m=\u001b[39m[X_y_train, X_y_valid],\n",
      "\u001b[0;32m    140\u001b[0m             eval_metric\u001b[39m=\u001b[39mloss_func,\n",
      "\u001b[0;32m    141\u001b[0m             verbose\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m\n",
      "\u001b[0;32m    142\u001b[0m         )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightgbm\\sklearn.py:840\u001b[0m, in \u001b[0;36mLGBMModel.best_iteration_\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    838\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\":obj:`int` or :obj:`None`: The best iteration of fitted model if ``early_stopping()`` callback has been specified.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__sklearn_is_fitted__():\n",
      "\u001b[1;32m--> 840\u001b[0m     \u001b[39mraise\u001b[39;00m LGBMNotFittedError(\u001b[39m'\u001b[39m\u001b[39mNo best_iteration found. Need to call fit with early_stopping callback beforehand.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_best_iteration\n",
      "\n",
      "\u001b[1;31mNotFittedError\u001b[0m: No best_iteration found. Need to call fit with early_stopping callback beforehand."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from home_credit.model.facade import predict_proba_facade\n",
    "\n",
    "# Create a dummy LGBMClassifier\n",
    "clf = LGBMClassifier()\n",
    "\n",
    "# Create a sample input dataset\n",
    "X_test = np.array([[5.1, 3.5, 1.4, 0.2], [6.2, 2.9, 4.3, 1.3]])\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_proba_facade(clf, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from home_credit.model.facade import predict_proba_facade\n",
    "\n",
    "# Create a dummy classifier that is not LGBM\n",
    "class DummyClassifier:\n",
    "    def predict_proba(self, X):\n",
    "        return np.array([[0.2, 0.8], [0.6, 0.4]])\n",
    "\n",
    "# Create a sample input dataset\n",
    "X_test = np.array([[1.1, 2.2], [3.3, 4.4]])\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict_proba_facade(DummyClassifier(), X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **`get_feat_imp_facade`**`(clf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from home_credit.model.facade import get_feature_importances_facade\n",
    "\n",
    "# Class attribute to store the Iris dataset\n",
    "iris = load_iris() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([336, 399, 661, 380])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Initialize a LightGBM model\n",
    "clf = LGBMClassifier()\n",
    "\n",
    "# Train the classifier with the Iris dataset\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Display the feature importances\n",
    "display(get_feature_importances_facade(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05941136, 0.01719151, 0.45463166, 0.46876547])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a RandomForestClassifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Train the classifier with the Iris dataset\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Display the feature importances\n",
    "display(get_feature_importances_facade(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.41795722,  0.96618456, -2.52142059, -1.08400771])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize a LogisticRegression model\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Train the classifier with the Iris dataset\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Display the feature importances\n",
    "display(get_feature_importances_facade(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier does not support feature importances. Use `[1.0 / n_features] * n_features` instead.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Initialize a DummyClassifier\n",
    "clf = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "# Train the classifier with the Iris dataset\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Display the feature importances\n",
    "try:\n",
    "    display(get_feature_importances_facade(clf))\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
