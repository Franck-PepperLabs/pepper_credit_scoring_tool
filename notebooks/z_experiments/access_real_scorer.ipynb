{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Problème** comment accéder à la fonction de scoring utilisée par un modèle\n",
    "\n",
    "Problème : pas d'introspection possible a priori du score utilisé par le modèle pour son entrainement.\n",
    "\n",
    "**La seule chose que l'on puisse faire est le réutiliser pour réévaluer une performance, mais sans savoir lequel il est avec certitude**\n",
    "\n",
    "Contexte d'apparition du problème :\n",
    "\n",
    "Je souhaitais effectuer une validation croisée d'abord sur la base du fonctionnement par défaut de `cross_val_score` qui utilise le scorer du modèle qui lui est passé en argument.\n",
    "\n",
    "Seulement voilà, pas moyen d'accéder à la fonction réelle qui est masquée par un `<bound method ClassifierMixin.score of TheModel()>`\n",
    "\n",
    "```Python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(scores)\n",
    "print(f\"{scores.mean()} {score_name} with a standard deviation of {scores.std()}\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentatives d'introspection (pour le moment en échec)\n",
    "\n",
    "Le seul champ qui m'apporte une information d'identification est le champ `__doc__`\n",
    "\n",
    "Je n'estime pas que cela soit suffisamment fiable, je veux pouvoir comparer la fonction sous-jacente wrappée directement avec ma collection de fonctions, les skl, comme toute fonction parsonnalisée qui pourrait avoir été définie.\n",
    "\n",
    "Fonctions qui pourraient être utiles :\n",
    "* `from sklearn.metrics import get_scorer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method ClassifierMixin.score of LogisticRegression()>\n",
      "<method-wrapper '__repr__' of method object at 0x000001E91F517100>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__call__': <method-wrapper '__call__' of method object at 0x000001E91F517100>,\n",
       " '__class__': method,\n",
       " '__delattr__': <method-wrapper '__delattr__' of method object at 0x000001E91F517100>,\n",
       " '__dir__': <function method.__dir__()>,\n",
       " '__doc__': '\\n        Return the mean accuracy on the given test data and labels.\\n\\n        In multi-label classification, this is the subset accuracy\\n        which is a harsh metric since you require for each sample that\\n        each label set be correctly predicted.\\n\\n        Parameters\\n        ----------\\n        X : array-like of shape (n_samples, n_features)\\n            Test samples.\\n\\n        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\\n            True labels for `X`.\\n\\n        sample_weight : array-like of shape (n_samples,), default=None\\n            Sample weights.\\n\\n        Returns\\n        -------\\n        score : float\\n            Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\\n        ',\n",
       " '__eq__': <method-wrapper '__eq__' of method object at 0x000001E91F517100>,\n",
       " '__format__': <function method.__format__(format_spec, /)>,\n",
       " '__func__': <function sklearn.base.ClassifierMixin.score(self, X, y, sample_weight=None)>,\n",
       " '__ge__': <method-wrapper '__ge__' of method object at 0x000001E91F517100>,\n",
       " '__getattribute__': <method-wrapper '__getattribute__' of method object at 0x000001E91F517100>,\n",
       " '__getstate__': <function method.__getstate__()>,\n",
       " '__gt__': <method-wrapper '__gt__' of method object at 0x000001E91F517100>,\n",
       " '__hash__': <method-wrapper '__hash__' of method object at 0x000001E91F517100>,\n",
       " '__init__': <method-wrapper '__init__' of method object at 0x000001E91F517100>,\n",
       " '__init_subclass__': <function method.__init_subclass__>,\n",
       " '__le__': <method-wrapper '__le__' of method object at 0x000001E91F517100>,\n",
       " '__lt__': <method-wrapper '__lt__' of method object at 0x000001E91F517100>,\n",
       " '__ne__': <method-wrapper '__ne__' of method object at 0x000001E91F517100>,\n",
       " '__new__': <function method.__new__(*args, **kwargs)>,\n",
       " '__reduce__': <function method.__reduce__()>,\n",
       " '__reduce_ex__': <function method.__reduce_ex__(protocol, /)>,\n",
       " '__repr__': <method-wrapper '__repr__' of method object at 0x000001E91F517100>,\n",
       " '__self__': LogisticRegression(),\n",
       " '__setattr__': <method-wrapper '__setattr__' of method object at 0x000001E91F517100>,\n",
       " '__sizeof__': <function method.__sizeof__()>,\n",
       " '__str__': <method-wrapper '__str__' of method object at 0x000001E91F517100>,\n",
       " '__subclasshook__': <function method.__subclasshook__>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "score_func = getattr(estimator, \"score\")\n",
    "if score_func is accuracy_score:\n",
    "    print(\"Estimator uses mean accuracy score\")\n",
    "# print(dir(score_func))\n",
    "print(score_func)\n",
    "print(score_func.__repr__)\n",
    "display({m: getattr(score_func, m) for m in dir(score_func)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py:794: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'explained_variance': make_scorer(explained_variance_score),\n",
       " 'r2': make_scorer(r2_score),\n",
       " 'max_error': make_scorer(max_error, greater_is_better=False),\n",
       " 'matthews_corrcoef': make_scorer(matthews_corrcoef),\n",
       " 'neg_median_absolute_error': make_scorer(median_absolute_error, greater_is_better=False),\n",
       " 'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),\n",
       " 'neg_mean_absolute_percentage_error': make_scorer(mean_absolute_percentage_error, greater_is_better=False),\n",
       " 'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),\n",
       " 'neg_mean_squared_log_error': make_scorer(mean_squared_log_error, greater_is_better=False),\n",
       " 'neg_root_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n",
       " 'neg_mean_poisson_deviance': make_scorer(mean_poisson_deviance, greater_is_better=False),\n",
       " 'neg_mean_gamma_deviance': make_scorer(mean_gamma_deviance, greater_is_better=False),\n",
       " 'accuracy': make_scorer(accuracy_score),\n",
       " 'top_k_accuracy': make_scorer(top_k_accuracy_score, needs_threshold=True),\n",
       " 'roc_auc': make_scorer(roc_auc_score, needs_threshold=True),\n",
       " 'roc_auc_ovr': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovr),\n",
       " 'roc_auc_ovo': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo),\n",
       " 'roc_auc_ovr_weighted': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovr, average=weighted),\n",
       " 'roc_auc_ovo_weighted': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo, average=weighted),\n",
       " 'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
       " 'average_precision': make_scorer(average_precision_score, needs_threshold=True),\n",
       " 'neg_log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       " 'neg_brier_score': make_scorer(brier_score_loss, greater_is_better=False, needs_proba=True),\n",
       " 'positive_likelihood_ratio': make_scorer(positive_likelihood_ratio),\n",
       " 'neg_negative_likelihood_ratio': make_scorer(negative_likelihood_ratio, greater_is_better=False),\n",
       " 'adjusted_rand_score': make_scorer(adjusted_rand_score),\n",
       " 'rand_score': make_scorer(rand_score),\n",
       " 'homogeneity_score': make_scorer(homogeneity_score),\n",
       " 'completeness_score': make_scorer(completeness_score),\n",
       " 'v_measure_score': make_scorer(v_measure_score),\n",
       " 'mutual_info_score': make_scorer(mutual_info_score),\n",
       " 'adjusted_mutual_info_score': make_scorer(adjusted_mutual_info_score),\n",
       " 'normalized_mutual_info_score': make_scorer(normalized_mutual_info_score),\n",
       " 'fowlkes_mallows_score': make_scorer(fowlkes_mallows_score),\n",
       " 'precision': make_scorer(precision_score, average=binary),\n",
       " 'precision_macro': make_scorer(precision_score, pos_label=None, average=macro),\n",
       " 'precision_micro': make_scorer(precision_score, pos_label=None, average=micro),\n",
       " 'precision_samples': make_scorer(precision_score, pos_label=None, average=samples),\n",
       " 'precision_weighted': make_scorer(precision_score, pos_label=None, average=weighted),\n",
       " 'recall': make_scorer(recall_score, average=binary),\n",
       " 'recall_macro': make_scorer(recall_score, pos_label=None, average=macro),\n",
       " 'recall_micro': make_scorer(recall_score, pos_label=None, average=micro),\n",
       " 'recall_samples': make_scorer(recall_score, pos_label=None, average=samples),\n",
       " 'recall_weighted': make_scorer(recall_score, pos_label=None, average=weighted),\n",
       " 'f1': make_scorer(f1_score, average=binary),\n",
       " 'f1_macro': make_scorer(f1_score, pos_label=None, average=macro),\n",
       " 'f1_micro': make_scorer(f1_score, pos_label=None, average=micro),\n",
       " 'f1_samples': make_scorer(f1_score, pos_label=None, average=samples),\n",
       " 'f1_weighted': make_scorer(f1_score, pos_label=None, average=weighted),\n",
       " 'jaccard': make_scorer(jaccard_score, average=binary),\n",
       " 'jaccard_macro': make_scorer(jaccard_score, pos_label=None, average=macro),\n",
       " 'jaccard_micro': make_scorer(jaccard_score, pos_label=None, average=micro),\n",
       " 'jaccard_samples': make_scorer(jaccard_score, pos_label=None, average=samples),\n",
       " 'jaccard_weighted': make_scorer(jaccard_score, pos_label=None, average=weighted)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "display(SCORERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
